{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c86ab335-dd3b-42b2-b972-0a62c6cd8fee",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Label'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Label'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mId\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprofileName\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview/time\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview/summary\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreviewLink\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfoLink\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Target Label (Assuming the dataset has a \"Label\" column for real/fake reviews)\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreal\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Split Data\u001b[39;00m\n\u001b[0;32m     50\u001b[0m X \u001b[38;5;241m=\u001b[39m data[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhelpfulness_ratio\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword_count\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment_score\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore_deviation\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Label'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8020f9fa-731d-44c5-8763-d2bb452b3571",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Label'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Label'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mId\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprofileName\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview/time\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview/summary\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreviewLink\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfoLink\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Target Label (Assuming the dataset has a \"Label\" column for real/fake reviews)\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreal\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Split Data\u001b[39;00m\n\u001b[0;32m     42\u001b[0m X \u001b[38;5;241m=\u001b[39m data[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhelpfulness_ratio\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword_count\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment_score\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore_deviation\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Label'"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "books_reviews = pd.read_csv(\"Books_rating.csv\")\n",
    "books_details = pd.read_csv(\"books_data.csv\")\n",
    "\n",
    "# Merge Data on Title\n",
    "data = pd.merge(books_reviews, books_details, on=\"Title\", how=\"inner\")\n",
    "\n",
    "# Feature Engineering\n",
    "# 1. Extract helpfulness ratio\n",
    "# Safely parse and calculate helpfulness ratio\n",
    "def calculate_helpfulness_ratio(value):\n",
    "    try:\n",
    "        # Ensure the value contains a fraction-like format (e.g., \"2/3\")\n",
    "        if isinstance(value, str) and '/' in value:\n",
    "            numerator, denominator = map(int, value.split('/'))\n",
    "            return numerator / max(denominator, 1)  # Avoid division by zero\n",
    "        else:\n",
    "            return 0  # Assign 0 if the format is invalid\n",
    "    except Exception:\n",
    "        return 0  # Fallback for any unexpected errors\n",
    "\n",
    "# Apply the function to the column\n",
    "data['helpfulness_ratio'] = data['review/helpfulness'].apply(calculate_helpfulness_ratio)\n",
    "\n",
    "\n",
    "# 2. Word Count in Review Text\n",
    "data['word_count'] = data['review/text'].fillna(\"\").apply(lambda x: len(x.split()))\n",
    "\n",
    "# 3. Sentiment Score (example placeholder)\n",
    "data['sentiment_score'] = data['review/text'].fillna(\"\").apply(lambda x: len([word for word in x.split() if word.lower() in ['good', 'excellent', 'bad', 'poor']]))\n",
    "\n",
    "# 4. Score Deviation\n",
    "data['score_deviation'] = abs(data['review/score'] - data['ratingsCount'].mean())\n",
    "\n",
    "# Dropping irrelevant columns\n",
    "data = data.drop(['Id', 'profileName', 'review/time', 'review/summary', 'image', 'previewLink', 'infoLink'], axis=1)\n",
    "\n",
    "# Target Label (Assuming the dataset has a \"Label\" column for real/fake reviews)\n",
    "data['Label'] = data['Label'].apply(lambda x: 1 if x == 'real' else 0)\n",
    "\n",
    "# Split Data\n",
    "X = data[['helpfulness_ratio', 'word_count', 'sentiment_score', 'score_deviation']]\n",
    "y = data['Label']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Text Vectorization (for review/text if used)\n",
    "tfidf = TfidfVectorizer(max_features=1000)\n",
    "X_text = tfidf.fit_transform(data['review/text'].fillna(\"\")).toarray()\n",
    "\n",
    "# Combine Text Features with Numeric Features\n",
    "X_train = np.hstack([X_train, X_text[:X_train.shape[0]]])\n",
    "X_val = np.hstack([X_val, X_text[X_train.shape[0]:]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "952fe61b-5951-4321-bfc5-418312f9c5c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 48\u001b[0m\n\u001b[0;32m     46\u001b[0m tfidf \u001b[38;5;241m=\u001b[39m TfidfVectorizer(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m     47\u001b[0m X_text_train \u001b[38;5;241m=\u001b[39m tfidf\u001b[38;5;241m.\u001b[39mfit_transform(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview/text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[1;32m---> 48\u001b[0m X_text_val \u001b[38;5;241m=\u001b[39m tfidf\u001b[38;5;241m.\u001b[39mtransform(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview/text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Combine Text Features with Numeric Features\u001b[39;00m\n\u001b[0;32m     51\u001b[0m X_train_combined \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack([X_train, X_text_train[:X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]]])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2115\u001b[0m, in \u001b[0;36mTfidfVectorizer.transform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   2098\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Transform documents to document-term matrix.\u001b[39;00m\n\u001b[0;32m   2099\u001b[0m \n\u001b[0;32m   2100\u001b[0m \u001b[38;5;124;03mUses the vocabulary and document frequencies (df) learned by fit (or\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2111\u001b[0m \u001b[38;5;124;03m    Tf-idf-weighted document-term matrix.\u001b[39;00m\n\u001b[0;32m   2112\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2113\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m, msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe TF-IDF vectorizer is not fitted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2115\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mtransform(raw_documents)\n\u001b[0;32m   2116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mtransform(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1417\u001b[0m, in \u001b[0;36mCountVectorizer.transform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1414\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_vocabulary()\n\u001b[0;32m   1416\u001b[0m \u001b[38;5;66;03m# use the same matrix-building strategy as fit_transform\u001b[39;00m\n\u001b[1;32m-> 1417\u001b[0m _, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_vocab(raw_documents, fixed_vocab\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1419\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load Data\n",
    "books_reviews = pd.read_csv(\"Books_rating.csv\")\n",
    "books_details = pd.read_csv(\"books_data.csv\")\n",
    "\n",
    "# Merge Data on Title\n",
    "data = pd.merge(books_reviews, books_details, on=\"Title\", how=\"inner\")\n",
    "\n",
    "# Preprocessing and Feature Engineering\n",
    "# 1. Extract helpfulness ratio\n",
    "def calculate_helpfulness_ratio(value):\n",
    "    try:\n",
    "        if isinstance(value, str) and '/' in value:\n",
    "            numerator, denominator = map(int, value.split('/'))\n",
    "            return numerator / max(denominator, 1)  # Avoid division by zero\n",
    "        else:\n",
    "            return 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "data['helpfulness_ratio'] = data['review/helpfulness'].apply(calculate_helpfulness_ratio)\n",
    "\n",
    "# 2. Word Count in Review Text\n",
    "data['word_count'] = data['review/text'].fillna(\"\").apply(lambda x: len(x.split()))\n",
    "\n",
    "# 3. Sentiment Score (example placeholder)\n",
    "data['sentiment_score'] = data['review/text'].fillna(\"\").apply(lambda x: len([word for word in x.split() if word.lower() in ['good', 'excellent', 'bad', 'poor']]))\n",
    "\n",
    "# 4. Score Deviation\n",
    "data['score_deviation'] = abs(data['review/score'] - data['ratingsCount'].mean())\n",
    "\n",
    "# Dropping irrelevant columns\n",
    "data = data.drop(['Id', 'profileName', 'review/time', 'review/summary', 'image', 'previewLink', 'infoLink'], axis=1, errors='ignore')\n",
    "\n",
    "# Simulate 'Label' column (replace this with real logic if available)\n",
    "np.random.seed(42)  # For reproducibility\n",
    "data['Label'] = np.random.choice([0, 1], size=len(data), p=[0.5, 0.5])  # 50% real, 50% fake\n",
    "\n",
    "# Split Data into Features and Target\n",
    "X = data[['helpfulness_ratio', 'word_count', 'sentiment_score', 'score_deviation']]\n",
    "y = data['Label']\n",
    "\n",
    "# Split into Train and Validation Sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Text Vectorization (for 'review/text')\n",
    "tfidf = TfidfVectorizer(max_features=1000)\n",
    "X_text_train = tfidf.fit_transform(data['review/text'].fillna(\"\")).toarray()\n",
    "X_text_val = tfidf.transform(data['review/text'].fillna(\"\")).toarray()\n",
    "\n",
    "# Combine Text Features with Numeric Features\n",
    "X_train_combined = np.hstack([X_train, X_text_train[:X_train.shape[0]]])\n",
    "X_val_combined = np.hstack([X_val, X_text_val[:X_val.shape[0]]])\n",
    "\n",
    "# Model Training\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train_combined, y_train)\n",
    "\n",
    "# Validation\n",
    "y_pred = clf.predict(X_val_combined)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(f\"Accuracy: {accuracy_score(y_val, y_pred):.2f}\")\n",
    "\n",
    "# Save Model and TFIDF Vectorizer\n",
    "with open(\"review_checker_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(clf, f)\n",
    "\n",
    "with open(\"tfidf_vectorizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tfidf, f)\n",
    "\n",
    "# Function to Predict New Data\n",
    "def predict_review(new_data):\n",
    "    # Load Model and Vectorizer\n",
    "    with open(\"review_checker_model.pkl\", \"rb\") as f:\n",
    "        model = pickle.load(f)\n",
    "    with open(\"tfidf_vectorizer.pkl\", \"rb\") as f:\n",
    "        tfidf = pickle.load(f)\n",
    "    \n",
    "    # Extract Features\n",
    "    new_data['helpfulness_ratio'] = new_data['review/helpfulness'].apply(calculate_helpfulness_ratio)\n",
    "    new_data['word_count'] = new_data['review/text'].fillna(\"\").apply(lambda x: len(x.split()))\n",
    "    new_data['sentiment_score'] = new_data['review/text'].fillna(\"\").apply(lambda x: len([word for word in x.split() if word.lower() in ['good', 'excellent', 'bad', 'poor']]))\n",
    "    new_data['score_deviation'] = abs(new_data['review/score'] - data['ratingsCount'].mean())\n",
    "    \n",
    "    # Text Vectorization\n",
    "    text_features = tfidf.transform(new_data['review/text'].fillna(\"\")).toarray()\n",
    "    \n",
    "    # Combine Features\n",
    "    numeric_features = new_data[['helpfulness_ratio', 'word_count', 'sentiment_score', 'score_deviation']].values\n",
    "    features = np.hstack([numeric_features, text_features])\n",
    "    \n",
    "    # Predict\n",
    "    predictions = model.predict(features)\n",
    "    return [\"Real\" if pred == 1 else \"Fake\" for pred in predictions]\n",
    "\n",
    "# Test Prediction\n",
    "new_test_data = pd.DataFrame({\n",
    "    \"review/helpfulness\": [\"2/3\"],\n",
    "    \"review/text\": [\"This is a great book! Highly recommended.\"],\n",
    "    \"review/score\": [5],\n",
    "    \"ratingsCount\": [500]\n",
    "})\n",
    "\n",
    "predictions = predict_review(new_test_data)\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f3615bf-83d0-494b-80ea-7729ce26c945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded successfully.\n",
      "Merging datasets on Title...\n",
      "Merged dataset contains 3000000 rows and 19 columns.\n",
      "Starting feature engineering...\n",
      "Calculated helpfulness ratio.\n",
      "Calculated word count for reviews.\n",
      "Calculated sentiment score.\n",
      "Calculated score deviation.\n",
      "Dropped irrelevant columns.\n",
      "Creating simulated 'Label' column...\n",
      "Simulated 'Label' column created.\n",
      "Splitting data into features and target...\n",
      "Data split into training and validation sets.\n",
      "Performing TF-IDF vectorization on review text...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 22.4 GiB for an array with shape (3000000, 1000) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 64\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerforming TF-IDF vectorization on review text...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     63\u001b[0m tfidf \u001b[38;5;241m=\u001b[39m TfidfVectorizer(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m---> 64\u001b[0m X_text_train \u001b[38;5;241m=\u001b[39m tfidf\u001b[38;5;241m.\u001b[39mfit_transform(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview/text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[0;32m     65\u001b[0m X_text_val \u001b[38;5;241m=\u001b[39m tfidf\u001b[38;5;241m.\u001b[39mtransform(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview/text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTF-IDF vectorization completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:1106\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m order \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1105\u001b[0m     order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swap(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcf\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 1106\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_toarray_args(order, out)\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mc_contiguous \u001b[38;5;129;01mor\u001b[39;00m out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mf_contiguous):\n\u001b[0;32m   1108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput array must be C or F contiguous\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_base.py:1327\u001b[0m, in \u001b[0;36m_spbase._process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39morder)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 22.4 GiB for an array with shape (3000000, 1000) and data type float64"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: Load Data\n",
    "print(\"Loading data...\")\n",
    "books_reviews = pd.read_csv(\"Books_rating.csv\")\n",
    "books_details = pd.read_csv(\"books_data.csv\")\n",
    "print(\"Data loaded successfully.\")\n",
    "\n",
    "# Step 2: Merge Data on Title\n",
    "print(\"Merging datasets on Title...\")\n",
    "data = pd.merge(books_reviews, books_details, on=\"Title\", how=\"inner\")\n",
    "print(f\"Merged dataset contains {len(data)} rows and {len(data.columns)} columns.\")\n",
    "\n",
    "# Step 3: Preprocessing and Feature Engineering\n",
    "print(\"Starting feature engineering...\")\n",
    "\n",
    "# 3.1 Extract helpfulness ratio\n",
    "def calculate_helpfulness_ratio(value):\n",
    "    try:\n",
    "        if isinstance(value, str) and '/' in value:\n",
    "            numerator, denominator = map(int, value.split('/'))\n",
    "            return numerator / max(denominator, 1)  # Avoid division by zero\n",
    "        else:\n",
    "            return 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "data['helpfulness_ratio'] = data['review/helpfulness'].apply(calculate_helpfulness_ratio)\n",
    "print(\"Calculated helpfulness ratio.\")\n",
    "\n",
    "# 3.2 Word Count in Review Text\n",
    "data['word_count'] = data['review/text'].fillna(\"\").apply(lambda x: len(x.split()))\n",
    "print(\"Calculated word count for reviews.\")\n",
    "\n",
    "# 3.3 Sentiment Score\n",
    "data['sentiment_score'] = data['review/text'].fillna(\"\").apply(\n",
    "    lambda x: len([word for word in x.split() if word.lower() in ['good', 'excellent', 'bad', 'poor']])\n",
    ")\n",
    "print(\"Calculated sentiment score.\")\n",
    "\n",
    "# 3.4 Score Deviation\n",
    "data['score_deviation'] = abs(data['review/score'] - data['ratingsCount'].mean())\n",
    "print(\"Calculated score deviation.\")\n",
    "\n",
    "# Dropping irrelevant columns\n",
    "columns_to_drop = ['Id', 'profileName', 'review/time', 'review/summary', 'image', 'previewLink', 'infoLink']\n",
    "data = data.drop(columns=columns_to_drop, axis=1, errors='ignore')\n",
    "print(\"Dropped irrelevant columns.\")\n",
    "\n",
    "# Step 4: Simulate 'Label' Column\n",
    "print(\"Creating simulated 'Label' column...\")\n",
    "np.random.seed(42)  # For reproducibility\n",
    "data['Label'] = np.random.choice([0, 1], size=len(data), p=[0.5, 0.5])  # 50% real, 50% fake\n",
    "print(\"Simulated 'Label' column created.\")\n",
    "\n",
    "# Step 5: Split Data into Features and Target\n",
    "print(\"Splitting data into features and target...\")\n",
    "X = data[['helpfulness_ratio', 'word_count', 'sentiment_score', 'score_deviation']]\n",
    "y = data['Label']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\"Data split into training and validation sets.\")\n",
    "\n",
    "# Step 6: Text Vectorization\n",
    "print(\"Performing TF-IDF vectorization on review text...\")\n",
    "tfidf = TfidfVectorizer(max_features=1000)\n",
    "X_text_train = tfidf.fit_transform(data['review/text'].fillna(\"\")).toarray()\n",
    "X_text_val = tfidf.transform(data['review/text'].fillna(\"\")).toarray()\n",
    "print(\"TF-IDF vectorization completed.\")\n",
    "\n",
    "# Combine Text Features with Numeric Features\n",
    "print(\"Combining text features with numeric features...\")\n",
    "X_train_combined = np.hstack([X_train, X_text_train[:X_train.shape[0]]])\n",
    "X_val_combined = np.hstack([X_val, X_text_val[:X_val.shape[0]]])\n",
    "print(\"Features combined successfully.\")\n",
    "\n",
    "# Step 7: Model Training\n",
    "print(\"Training Random Forest Classifier...\")\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train_combined, y_train)\n",
    "print(\"Model training completed.\")\n",
    "\n",
    "# Step 8: Validation\n",
    "print(\"Validating the model...\")\n",
    "y_pred = clf.predict(X_val_combined)\n",
    "print(\"Validation completed. Results:\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(f\"Accuracy: {accuracy_score(y_val, y_pred):.2f}\")\n",
    "\n",
    "# Step 9: Save Model and Vectorizer\n",
    "print(\"Saving the model and TF-IDF vectorizer...\")\n",
    "with open(\"review_checker_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(clf, f)\n",
    "\n",
    "with open(\"tfidf_vectorizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tfidf, f)\n",
    "print(\"Model and vectorizer saved successfully.\")\n",
    "\n",
    "# Step 10: Predict New Data\n",
    "def predict_review(new_data):\n",
    "    print(\"Loading model and vectorizer for prediction...\")\n",
    "    with open(\"review_checker_model.pkl\", \"rb\") as f:\n",
    "        model = pickle.load(f)\n",
    "    with open(\"tfidf_vectorizer.pkl\", \"rb\") as f:\n",
    "        tfidf = pickle.load(f)\n",
    "    print(\"Model and vectorizer loaded.\")\n",
    "\n",
    "    # Extract Features\n",
    "    print(\"Extracting features from new data...\")\n",
    "    new_data['helpfulness_ratio'] = new_data['review/helpfulness'].apply(calculate_helpfulness_ratio)\n",
    "    new_data['word_count'] = new_data['review/text'].fillna(\"\").apply(lambda x: len(x.split()))\n",
    "    new_data['sentiment_score'] = new_data['review/text'].fillna(\"\").apply(\n",
    "        lambda x: len([word for word in x.split() if word.lower() in ['good', 'excellent', 'bad', 'poor']])\n",
    "    )\n",
    "    new_data['score_deviation'] = abs(new_data['review/score'] - data['ratingsCount'].mean())\n",
    "    \n",
    "    # Text Vectorization\n",
    "    text_features = tfidf.transform(new_data['review/text'].fillna(\"\")).toarray()\n",
    "    \n",
    "    # Combine Features\n",
    "    numeric_features = new_data[['helpfulness_ratio', 'word_count', 'sentiment_score', 'score_deviation']].values\n",
    "    features = np.hstack([numeric_features, text_features])\n",
    "    print(\"Features extracted.\")\n",
    "\n",
    "    # Predict\n",
    "    print(\"Making predictions...\")\n",
    "    predictions = model.predict(features)\n",
    "    return [\"Real\" if pred == 1 else \"Fake\" for pred in predictions]\n",
    "\n",
    "# Test Prediction\n",
    "print(\"Testing prediction on new data...\")\n",
    "new_test_data = pd.DataFrame({\n",
    "    \"review/helpfulness\": [\"2/3\"],\n",
    "    \"review/text\": [\"This is a great book! Highly recommended.\"],\n",
    "    \"review/score\": [5],\n",
    "    \"ratingsCount\": [500]\n",
    "})\n",
    "predictions = predict_review(new_test_data)\n",
    "print(\"Prediction result:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c7faf4-6912-49b7-9d7a-1b331d72f91e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded successfully.\n",
      "Merging datasets on Title...\n",
      "Merged dataset contains 3000000 rows and 19 columns.\n",
      "Starting feature engineering...\n",
      "Calculated helpfulness ratio.\n",
      "Calculated word count for reviews.\n",
      "Calculated sentiment score.\n",
      "Calculated score deviation.\n",
      "Dropped irrelevant columns.\n",
      "Creating simulated 'Label' column...\n",
      "Simulated 'Label' column created.\n",
      "Splitting data into features and target...\n",
      "Data split into training and validation sets.\n",
      "Performing TF-IDF vectorization on review text...\n",
      "TF-IDF vectorization completed.\n",
      "Combining text features with numeric features...\n",
      "Features combined successfully.\n",
      "Training Random Forest Classifier...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Step 1: Load Data\n",
    "print(\"Loading data...\")\n",
    "books_reviews = pd.read_csv(\"Books_rating.csv\")\n",
    "books_details = pd.read_csv(\"books_data.csv\")\n",
    "print(\"Data loaded successfully.\")\n",
    "\n",
    "# Step 2: Merge Data on Title\n",
    "print(\"Merging datasets on Title...\")\n",
    "data = pd.merge(books_reviews, books_details, on=\"Title\", how=\"inner\")\n",
    "print(f\"Merged dataset contains {len(data)} rows and {len(data.columns)} columns.\")\n",
    "\n",
    "# Step 3: Preprocessing and Feature Engineering\n",
    "print(\"Starting feature engineering...\")\n",
    "\n",
    "# 3.1 Extract helpfulness ratio\n",
    "def calculate_helpfulness_ratio(value):\n",
    "    try:\n",
    "        if isinstance(value, str) and '/' in value:\n",
    "            numerator, denominator = map(int, value.split('/'))\n",
    "            return numerator / max(denominator, 1)  # Avoid division by zero\n",
    "        else:\n",
    "            return 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "data['helpfulness_ratio'] = data['review/helpfulness'].apply(calculate_helpfulness_ratio)\n",
    "print(\"Calculated helpfulness ratio.\")\n",
    "\n",
    "# 3.2 Word Count in Review Text\n",
    "data['word_count'] = data['review/text'].fillna(\"\").apply(lambda x: len(x.split()))\n",
    "print(\"Calculated word count for reviews.\")\n",
    "\n",
    "# 3.3 Sentiment Score\n",
    "data['sentiment_score'] = data['review/text'].fillna(\"\").apply(\n",
    "    lambda x: len([word for word in x.split() if word.lower() in ['good', 'excellent', 'bad', 'poor']])\n",
    ")\n",
    "print(\"Calculated sentiment score.\")\n",
    "\n",
    "# 3.4 Score Deviation\n",
    "data['score_deviation'] = abs(data['review/score'] - data['ratingsCount'].mean())\n",
    "print(\"Calculated score deviation.\")\n",
    "\n",
    "# Dropping irrelevant columns\n",
    "columns_to_drop = ['Id', 'profileName', 'review/time', 'review/summary', 'image', 'previewLink', 'infoLink']\n",
    "data = data.drop(columns=columns_to_drop, axis=1, errors='ignore')\n",
    "print(\"Dropped irrelevant columns.\")\n",
    "\n",
    "# Step 4: Simulate 'Label' Column\n",
    "print(\"Creating simulated 'Label' column...\")\n",
    "np.random.seed(42)  # For reproducibility\n",
    "data['Label'] = np.random.choice([0, 1], size=len(data), p=[0.5, 0.5])  # 50% real, 50% fake\n",
    "print(\"Simulated 'Label' column created.\")\n",
    "\n",
    "# Step 5: Split Data into Features and Target\n",
    "print(\"Splitting data into features and target...\")\n",
    "X_numeric = data[['helpfulness_ratio', 'word_count', 'sentiment_score', 'score_deviation']]\n",
    "y = data['Label']\n",
    "X_numeric_train, X_numeric_val, y_train, y_val = train_test_split(X_numeric, y, test_size=0.2, random_state=42)\n",
    "print(\"Data split into training and validation sets.\")\n",
    "\n",
    "# Step 6: Text Vectorization\n",
    "print(\"Performing TF-IDF vectorization on review text...\")\n",
    "tfidf = TfidfVectorizer(max_features=1000)\n",
    "X_text_sparse = tfidf.fit_transform(data['review/text'].fillna(\"\"))\n",
    "X_text_train = X_text_sparse[:X_numeric_train.shape[0]]\n",
    "X_text_val = X_text_sparse[X_numeric_train.shape[0]:]\n",
    "print(\"TF-IDF vectorization completed.\")\n",
    "\n",
    "# Combine Text Features with Numeric Features (Sparse Matrices)\n",
    "from scipy.sparse import hstack\n",
    "print(\"Combining text features with numeric features...\")\n",
    "X_train_combined = hstack([X_numeric_train, X_text_train])\n",
    "X_val_combined = hstack([X_numeric_val, X_text_val])\n",
    "print(\"Features combined successfully.\")\n",
    "\n",
    "# Step 7: Model Training\n",
    "print(\"Training Random Forest Classifier...\")\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train_combined, y_train)\n",
    "print(\"Model training completed.\")\n",
    "\n",
    "# Step 8: Validation\n",
    "print(\"Validating the model...\")\n",
    "y_pred = clf.predict(X_val_combined)\n",
    "print(\"Validation completed. Results:\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(f\"Accuracy: {accuracy_score(y_val, y_pred):.2f}\")\n",
    "\n",
    "# Step 9: Save Model and Vectorizer\n",
    "print(\"Saving the model and TF-IDF vectorizer...\")\n",
    "with open(\"review_checker_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(clf, f)\n",
    "\n",
    "with open(\"tfidf_vectorizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tfidf, f)\n",
    "print(\"Model and vectorizer saved successfully.\")\n",
    "\n",
    "# Step 10: Predict New Data\n",
    "def predict_review(new_data):\n",
    "    print(\"Loading model and vectorizer for prediction...\")\n",
    "    with open(\"review_checker_model.pkl\", \"rb\") as f:\n",
    "        model = pickle.load(f)\n",
    "    with open(\"tfidf_vectorizer.pkl\", \"rb\") as f:\n",
    "        tfidf = pickle.load(f)\n",
    "    print(\"Model and vectorizer loaded.\")\n",
    "\n",
    "    # Extract Features\n",
    "    print(\"Extracting features from new data...\")\n",
    "    new_data['helpfulness_ratio'] = new_data['review/helpfulness'].apply(calculate_helpfulness_ratio)\n",
    "    new_data['word_count'] = new_data['review/text'].fillna(\"\").apply(lambda x: len(x.split()))\n",
    "    new_data['sentiment_score'] = new_data['review/text'].fillna(\"\").apply(\n",
    "        lambda x: len([word for word in x.split() if word.lower() in ['good', 'excellent', 'bad', 'poor']])\n",
    "    )\n",
    "    new_data['score_deviation'] = abs(new_data['review/score'] - data['ratingsCount'].mean())\n",
    "    \n",
    "    # Text Vectorization\n",
    "    text_features = tfidf.transform(new_data['review/text'].fillna(\"\"))\n",
    "    \n",
    "    # Combine Features (Sparse Matrices)\n",
    "    numeric_features = new_data[['helpfulness_ratio', 'word_count', 'sentiment_score', 'score_deviation']].values\n",
    "    numeric_sparse = hstack([numeric_features])  # Convert to sparse\n",
    "    features = hstack([numeric_sparse, text_features])\n",
    "    print(\"Features extracted.\")\n",
    "\n",
    "    # Predict\n",
    "    print(\"Making predictions...\")\n",
    "    predictions = model.predict(features)\n",
    "    return [\"Real\" if pred == 1 else \"Fake\" for pred in predictions]\n",
    "\n",
    "# Test Prediction\n",
    "print(\"Testing prediction on new data...\")\n",
    "new_test_data = pd.DataFrame({\n",
    "    \"review/helpfulness\": [\"2/3\"],\n",
    "    \"review/text\": [\"This is a great book! Highly recommended.\"],\n",
    "    \"review/score\": [5],\n",
    "    \"ratingsCount\": [500]\n",
    "})\n",
    "predictions = predict_review(new_test_data)\n",
    "print(\"Prediction result:\", predictions)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e244e447-d9fc-4784-b547-e13b8265075c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
